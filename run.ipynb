{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d0b9fed-44ac-4bc3-ab6b-a5514f80074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from implementations import *\n",
    "import importlib\n",
    "from preprocess_data import *\n",
    "from cross_validation import *\n",
    "from utils import *\n",
    "from predict import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e26e4e2-3ea0-458e-a27d-63cf60214aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data('data/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1a4419c8-f562-42c9-a4dd-51391d61a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/dataset_to_release/\"\n",
    "\n",
    "\n",
    "def model(x_train, y_train, gammas = [0.001, 0.01], lambdas = [0.01, 0.03]):\n",
    "    \"\"\"\n",
    "    Train a regularized logistic regression model using cross-validation to find the best hyperparameters.\n",
    "    Train the model with the best hyper parameters on the given dataset\n",
    "\n",
    "    Parameters:\n",
    "    tx (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
    "    y (numpy.ndarray): Label vector of shape (num_samples,).\n",
    "    max_iters (int): Maximum number of iterations for the logistic regression optimization.\n",
    "    gammas (list): List of learning rate values to search through during cross-validation.\n",
    "    lambdas (list): List of lambda (regularization strength) values to search through during cross-validation.\n",
    "    regulizer_orders (list): List of regularization orders to search through during cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    w (numpy.ndarray): Weight vector of the trained logistic regression model.\n",
    "    loss (float): Loss value of the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    functions = [\"least_squares\", \"mean_squared_error_gd\", \"mean_squared_error_sgd\", \"ridge_regression\", \"logistic_regression\",  \"reg_logistic_regression\"]\n",
    "    cur_functions = [\"least_squares\", \"mean_squared_error_sgd\", \"ridge_regression\", \"logistic_regression\",  \"reg_logistic_regression\"]\n",
    "    best_hyper_params = []\n",
    "\n",
    "    \n",
    "    print(\"using least_squares\")\n",
    "    best_hyper_params.append(\n",
    "        hypertuning(y_train, x_train, [0], [0], functions[0], np.arange(0, 0.3, 0.01))\n",
    "    )\n",
    "    \n",
    "    #print(\"using mean_squared_error_gd\")\n",
    "    #best_hyper_params.append(\n",
    "    #    hypertuning(y_train, x_train, gammas, [0], functions[1])\n",
    "    #)\n",
    "\n",
    "    print(\"using mean_squared_error_sgd\")\n",
    "    best_hyper_params.append(\n",
    "        hypertuning(y_train, x_train, gammas, [0], functions[2], np.arange(0, 0.3, 0.01))\n",
    "    )\n",
    "\n",
    "    print(\"using ridge linear regression\")\n",
    "    best_hyper_params.append(\n",
    "        hypertuning(y_train, x_train, [0], lambdas, functions[3], np.arange(0, 0.3, 0.01))\n",
    "    )\n",
    "\n",
    "    print(\"using logistic regression\")\n",
    "    best_hyper_params.append(\n",
    "        hypertuning(y_train, x_train, gammas, [0], functions[4], np.arange(0.4, 0.6, 0.01))\n",
    "    )\n",
    "\n",
    "    print(\"using regularized logistic regression\")\n",
    "    best_hyper_params.append(\n",
    "       hypertuning(y_train, x_train, gammas, lambdas, functions[5], [0.5])\n",
    "    )\n",
    "\n",
    "    perfs = [x[3][2] for x in best_hyper_params]\n",
    "    best_np_hyperparams = [x[:3] for x in best_hyper_params]\n",
    "    best_arr_hyper_params = np.array(best_np_hyperparams)\n",
    "\n",
    "    # Determine the best model based on a combination of F1 score and accuracy\n",
    "    best_idx = np.argmax(perfs)\n",
    "    best_lambda = best_arr_hyper_params[best_idx, 0]\n",
    "    best_gamma = best_arr_hyper_params[best_idx, 1]\n",
    "    best_threshold = best_arr_hyper_params[best_idx, 2]\n",
    "    best_model = cur_functions[best_idx]\n",
    "\n",
    "\n",
    "    \n",
    "    x_train_proc, x_test_proc, y_train_proc = preprocess(x_train, x_test, y_train, best_model)\n",
    "    w = 0\n",
    "    # Train the model using the specified method\n",
    "    if best_model == \"mean_squared_error_gd\":\n",
    "        w, _ = mean_squared_error_gd(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "    elif best_model == \"mean_squared_error_sgd\":\n",
    "        w, _ = mean_squared_error_sgd(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "    elif best_model == \"least_squares\":\n",
    "        w, _ = least_squares(y_train_proc, x_train_proc)\n",
    "    elif best_model == \"ridge_regression\":\n",
    "        w, _ = ridge_regression(y_train_proc, x_train_proc, best_lambda)\n",
    "    elif best_model == \"logistic_regression\":\n",
    "        w, _ = logistic_regression(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "    elif best_model == \"reg_logistic_regression\":\n",
    "        w, _ = reg_logistic_regression(y_train_proc, x_train_proc, best_lambda, initial_w, max_iters, best_gamma)\n",
    "    \n",
    "    return w, best_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f42c2-e9ff-44d9-b6e5-769970380038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "using regularized logistic regression\n",
      "Labels converted from -1 to 0.\n",
      "Preprocessing for fold 1:\n",
      "\n",
      "Labels converted from -1 to 0.\n",
      "6 features where all values are the same removed.\n",
      "Replaced all NaN values with -1.\n",
      "Removed 3698 outliers (z-score > 2).\n",
      "Data standardized using z-score scaling.\n",
      "Original sizes:\n",
      "  - Majority class (0): 146297 (91.23%)\n",
      "  - Minority class (1): 14072 (8.77%)\n",
      "Minority class upsampled:\n",
      "  - Class (0): 146297 (80.00%)\n",
      "  - Class (1): 36574 (20.00%)\n",
      "PCA performed to reduce features from 315 to 299.\n",
      "Labels converted from -1 to 0.\n",
      "Preprocessing for fold 2:\n",
      "\n",
      "Labels converted from -1 to 0.\n",
      "6 features where all values are the same removed.\n",
      "Replaced all NaN values with -1.\n",
      "Removed 2908 outliers (z-score > 2).\n",
      "Data standardized using z-score scaling.\n",
      "Original sizes:\n",
      "  - Majority class (0): 146855 (91.12%)\n",
      "  - Minority class (1): 14305 (8.88%)\n",
      "Minority class upsampled:\n",
      "  - Class (0): 146855 (80.00%)\n",
      "  - Class (1): 36713 (20.00%)\n",
      "PCA performed to reduce features from 315 to 299.\n",
      " lambda = 0.0001, gamma = 0.0001, decision_threshold = 0.5: F1 = 0.288 and acc = 0.625\n"
     ]
    }
   ],
   "source": [
    "## Define hyperparameter search spaces\n",
    "print(\"Start training\")\n",
    "lambdas = np.logspace(-4, 3, 8)\n",
    "gammas = np.logspace(-4, 3, 8)\n",
    "\n",
    "# Train the model and find the best hyperparameters\n",
    "w, best_model = model(x_train, y_train, gammas, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8bc1b22-4a6e-42c9-ae98-1b960515d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels converted from -1 to 0.\n",
      "6 features where all values are the same removed.\n",
      "Replaced all NaN values with -1.\n",
      "Data standardized using z-score scaling.\n",
      "Original sizes:\n",
      "  - Majority class (0): 299160 (91.17%)\n",
      "  - Minority class (1): 28975 (8.83%)\n",
      "Minority class upsampled:\n",
      "  - Class (0): 299160 (80.00%)\n",
      "  - Class (1): 74790 (20.00%)\n",
      "PCA performed to reduce features from 315 to 299.\n"
     ]
    }
   ],
   "source": [
    "x_train_proc, x_test_proc, y_train_proc = preprocess(x_train, x_test, y_train, best_model)\n",
    "w = 0\n",
    "# Train the model using the specified method\n",
    "if best_model == \"mean_squared_error_gd\":\n",
    "    w, _ = mean_squared_error_gd(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "elif best_model == \"mean_squared_error_sgd\":\n",
    "    w, _ = mean_squared_error_sgd(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "elif best_model == \"least_squares\":\n",
    "    w, _ = least_squares(y_train_proc, x_train_proc)\n",
    "elif best_model == \"ridge_regression\":\n",
    "    w, _ = ridge_regression(y_train_proc, x_train_proc, best_lambda)\n",
    "elif best_model == \"logistic_regression\":\n",
    "    w, _ = logistic_regression(y_train_proc, x_train_proc, initial_w, max_iters, best_gamma)\n",
    "elif best_model == \"reg_logistic_regression\":\n",
    "    w, _ = reg_logistic_regression(y_train_proc, x_train_proc, best_lambda, initial_w, max_iters, best_gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a4aa0de-bcf9-41a6-86b4-615e06335b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is  ridge_regression\n",
      "Labels converted from -1 to 0.\n",
      "6 features where all values are the same removed.\n",
      "Replaced all NaN values with -1.\n",
      "Data standardized using z-score scaling.\n",
      "Original sizes:\n",
      "  - Majority class (0): 299160 (91.17%)\n",
      "  - Minority class (1): 28975 (8.83%)\n",
      "Minority class upsampled:\n",
      "  - Class (0): 299160 (80.00%)\n",
      "  - Class (1): 74790 (20.00%)\n",
      "PCA performed to reduce features from 315 to 299.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model parameters\n",
    "print(\"The best model is \", best_model)\n",
    "np.save(\"model_performance/model_parameters.npy\", w)\n",
    "\n",
    "# Perform classification on the predictions and save the results\n",
    "x_train_proc, x_test_proc, y_train_proc = preprocess(x_train, x_test, y_train, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9900ed5e-d49c-4e63-9bce-5e5c2b746c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ... -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(w, x_test_proc, best_model, threshold = 0)\n",
    "\n",
    "y_pred = np.where(y_pred==0, -1, 1)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Use the create_csv_submission function to save predictions\n",
    "create_csv_submission(test_ids, y_pred, \"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60fdac-8217-41e3-bdf8-b1be80704154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
